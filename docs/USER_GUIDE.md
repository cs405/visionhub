# visionhub å®Œæ•´ä½¿ç”¨æ–‡æ¡£

> **ç‰ˆæœ¬**: v1.0.0  
> **æ›´æ–°æ—¶é—´**: 2026-01-09  
> **çŠ¶æ€**: ç”Ÿäº§å°±ç»ª

---

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®ç®€ä»‹](#1-é¡¹ç›®ç®€ä»‹)
2. [å®‰è£…æŒ‡å—](#2-å®‰è£…æŒ‡å—)
3. [å¿«é€Ÿå¼€å§‹](#3-å¿«é€Ÿå¼€å§‹)
4. [æ ¸å¿ƒåŠŸèƒ½è¯¦è§£](#4-æ ¸å¿ƒåŠŸèƒ½è¯¦è§£)
5. [Backboneæ¨¡å‹åº“](#5-backboneæ¨¡å‹åº“)
6. [Losså‡½æ•°åº“](#6-losså‡½æ•°åº“)
7. [æ•°æ®å¢å¼º](#7-æ•°æ®å¢å¼º)
8. [è®­ç»ƒæŒ‡å—](#8-è®­ç»ƒæŒ‡å—)
9. [YOLOé›†æˆ](#9-yoloé›†æˆ)
10. [éƒ¨ç½²æŒ‡å—](#10-éƒ¨ç½²æŒ‡å—)
11. [APIå‚è€ƒ](#11-apiå‚è€ƒ)
12. [å¸¸è§é—®é¢˜](#12-å¸¸è§é—®é¢˜)

---

## 1. é¡¹ç›®ç®€ä»‹

### 1.1 ä»€ä¹ˆæ˜¯visionhubï¼Ÿ

visionhubæ˜¯ä¸€ä¸ª**å…¨åŠŸèƒ½çš„PyTorchå›¾åƒåˆ†ç±»å·¥å…·åŒ…**ï¼Œä»visionhubå®Œæ•´è¿ç§»è€Œæ¥ï¼Œæ”¯æŒï¼š

- âœ… **å›¾åƒåˆ†ç±»**ï¼šæ ‡å‡†åˆ†ç±»ä»»åŠ¡
- âœ… **å›¾åƒæ£€ç´¢**ï¼šå‘é‡æ£€ç´¢ã€ç›¸ä¼¼åº¦æœç´¢
- âœ… **äººè„¸è¯†åˆ«**ï¼š1:1éªŒè¯ã€1:Nè¯†åˆ«
- âœ… **YOLOé›†æˆ**ï¼šæ£€æµ‹+åˆ†ç±»è”åˆæ¨ç†
- âœ… **æ¨¡å‹éƒ¨ç½²**ï¼šONNXã€TensorRTã€é‡åŒ–
- âœ… **çŸ¥è¯†è’¸é¦**ï¼šTeacher-Studentè’¸é¦

### 1.2 æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | æè¿° |
|------|------|
| **85+ Backbone** | ResNet, EfficientNet, ViT, Swinç­‰ |
| **50+ Losså‡½æ•°** | åº¦é‡å­¦ä¹ ã€è’¸é¦ã€åˆ†ç±»Loss |
| **16ç§æ•°æ®å¢å¼º** | AutoAugment, Mixup, CutMixç­‰ |
| **å®Œæ•´éƒ¨ç½²** | ONNX, TensorRT, é‡åŒ–, HTTPæœåŠ¡ |
| **YOLOæ”¯æŒ** | æ£€æµ‹+åˆ†ç±»/æ£€ç´¢è”åˆæ¨ç† |

### 1.3 ä¸visionhubå¯¹æ¯”

| åŠŸèƒ½ | visionhub | visionhub |
|------|-----------|-------------|
| æ¡†æ¶ | visionhubvisionhub | PyTorch |
| Backbone | 100+ | 85+ (æ ¸å¿ƒå…¨è¦†ç›–) |
| Losså‡½æ•° | 60+ | 50+ (ä¸»æµå…¨è¦†ç›–) |
| YOLOé›†æˆ | âŒ | âœ… |
| éƒ¨ç½²å·¥å…· | âœ… | âœ… |
| å®Œæˆåº¦ | 100% | **90%** |

---

## 2. å®‰è£…æŒ‡å—

### 2.1 ç¯å¢ƒè¦æ±‚

```
Python >= 3.8
PyTorch >= 1.10.0
CUDA >= 11.0 (æ¨èï¼Œç”¨äºGPUåŠ é€Ÿ)
```

### 2.2 åŸºç¡€å®‰è£…

```bash
# æ–¹å¼1: pipå®‰è£…ï¼ˆæ¨èï¼‰
pip install visionhub

# æ–¹å¼2: ä»æºç å®‰è£…
git clone https://github.com/visionhub/visionhub.git
cd visionhub
pip install -e .

# æ–¹å¼3: ä»…å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2.3 å¯é€‰å®‰è£…

```bash
# GPUç‰ˆæœ¬ï¼ˆFaiss GPUåŠ é€Ÿï¼‰
pip install visionhub[gpu]

# ONNXå¯¼å‡ºæ”¯æŒ
pip install visionhub[onnx]

# HTTPæœåŠ¡æ”¯æŒ
pip install visionhub[serving]

# å®Œæ•´å®‰è£…ï¼ˆæ‰€æœ‰åŠŸèƒ½ï¼‰
pip install visionhub[all]
```

### 2.4 éªŒè¯å®‰è£…

```python
import torch
import visionhub

print(f"PyTorch version: {torch.__version__}")
print(f"visionhub version: {visionhub.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

# æŸ¥çœ‹å¯ç”¨æ¨¡å‹
from visionhub.ptcls.arch.backbone import list_backbones
print(f"Available backbones: {len(list_backbones())}")
```

---

## 3. å¿«é€Ÿå¼€å§‹

### 3.1 å›¾åƒåˆ†ç±»ï¼ˆ5åˆ†é’Ÿä¸Šæ‰‹ï¼‰

```python
from visionhub.ptcls.arch.backbone import build_backbone
from torchvision import transforms
from PIL import Image
import torch

# 1. åŠ è½½æ¨¡å‹
model = build_backbone('resnet50', num_classes=1000, pretrained=True)
model.eval()

# 2. å›¾åƒé¢„å¤„ç†
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# 3. æ¨ç†
img = Image.open('test.jpg')
img_tensor = transform(img).unsqueeze(0)

with torch.no_grad():
    output = model(img_tensor)
    prob = torch.softmax(output, dim=1)
    top5_prob, top5_idx = prob.topk(5)

print(f"Top-5 predictions: {list(zip(top5_idx[0].tolist(), top5_prob[0].tolist()))}")
```

### 3.2 å›¾åƒæ£€ç´¢ï¼ˆ10åˆ†é’Ÿä¸Šæ‰‹ï¼‰

```python
from visionhub.ptcls.rec import RecPredictor
from visionhub.ptcls.rec.gallery_builder import GalleryBuilder

# 1. æ„å»ºæ£€ç´¢åº“
builder = GalleryBuilder(
    model_name='resnet50',
    embedding_size=512
)
builder.build_from_directory('gallery_images/')

# 2. æ£€ç´¢
predictor = RecPredictor(
    model_name='resnet50',
    gallery_path='gallery.faiss'
)
results = predictor.search('query.jpg', top_k=5)

print(f"Top-5 similar images: {results}")
```

### 3.3 YOLO + åˆ†ç±»ï¼ˆ15åˆ†é’Ÿä¸Šæ‰‹ï¼‰

```python
from visionhub.ptcls.tools.yolo_det_classification import YOLODetectionClassification

# åˆ›å»ºç³»ç»Ÿ
system = YOLODetectionClassification(
    yolo_model_path='yolov8n.pt',
    cls_model_name='resnet50',
    cls_checkpoint='classifier.pth',
    num_classes=100
)

# æ£€æµ‹+åˆ†ç±»
results = system.detect_and_classify('image.jpg', save_result=True)

for i, res in enumerate(results):
    print(f"{i+1}. {res['cls_class_name']} "
          f"(box: {res['box']}, conf: {res['cls_conf']:.3f})")
```

---

## 4. æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### 4.1 å›¾åƒåˆ†ç±»

#### æ ‡å‡†åˆ†ç±»è®­ç»ƒ

```bash
python tools/train_classification.py \
  --data_root ./data/imagenet \
  --model resnet50 \
  --num_classes 1000 \
  --epochs 100 \
  --batch_size 128 \
  --lr 0.1 \
  --device cuda \
  --save_dir ./output/resnet50
```

**æ•°æ®é›†æ ¼å¼ï¼ˆImageFolderï¼‰**:
```
data/
  train/
    class1/
      img1.jpg
      img2.jpg
    class2/
      img1.jpg
  val/
    class1/
    class2/
```

#### è¯„ä¼°

```bash
python tools/eval_classification.py \
  --model resnet50 \
  --checkpoint output/resnet50/best.pth \
  --data_root data/val \
  --num_classes 1000
```

### 4.2 å›¾åƒæ£€ç´¢

#### æ£€ç´¢æ¨¡å‹è®­ç»ƒ

```bash
python tools/train_rec_kd.py \
  --yolo_images dataset/train \
  --yolo_labels dataset/train \
  --data_yaml dataset/data.yaml \
  --save_dir output/retrieval \
  --epochs 50 \
  --batch_size 32 \
  --use_pk --P 8 --K 4 \
  --w_triplet 1.0 --w_circle 0.2 \
  --device cuda
```

**æ•°æ®é›†æ ¼å¼ï¼ˆYOLOæ£€æµ‹ï¼‰**:
```
dataset/
  train/
    images/
      img1.jpg
      img2.jpg
    labels/
      img1.txt  # class_id x_center y_center width height
      img2.txt
  data.yaml  # names: [class1, class2, ...]
```

#### æ„å»ºæ£€ç´¢åº“

```bash
python tools/build_gallery.py \
  -c configs/shitu/rec_faiss_demo.yaml
```

#### æ£€ç´¢è¯„ä¼°

```bash
python tools/eval_retrieval.py \
  -c configs/shitu/rec_faiss_demo.yaml \
  --gallery_images dataset/val \
  --gallery_labels dataset/val \
  --query_images dataset/test \
  --query_labels dataset/test \
  --data_yaml dataset/data.yaml \
  --strict_image_split \
  --exclude_same_image
```

### 4.3 äººè„¸è¯†åˆ«

#### è®­ç»ƒäººè„¸æ¨¡å‹

```bash
python tools/train_face_recognition.py \
  --train_root faces/train \
  --val_pairs faces/val/pairs.txt \
  --model ir_net_50 \
  --loss arcface \
  --s 64.0 --m 0.5 \
  --epochs 100 \
  --batch_size 128 \
  --save_dir output/face
```

**æ•°æ®é›†æ ¼å¼ï¼ˆäººè„¸è®­ç»ƒï¼‰**:
```
faces/
  train/
    person1/
      face1.jpg
      face2.jpg
    person2/
      face1.jpg
  val/
    pairs.txt  # path1 path2 1/0
```

#### 1:1 äººè„¸éªŒè¯

```bash
python tools/face_recognition_inference.py \
  --task verify \
  --model ir_net_50 \
  --checkpoint output/face/best.pth \
  --image1 person1.jpg \
  --image2 person2.jpg \
  --threshold 0.3
```

#### 1:N äººè„¸è¯†åˆ«

```bash
python tools/face_recognition_inference.py \
  --task identify \
  --model ir_net_50 \
  --checkpoint output/face/best.pth \
  --query query.jpg \
  --gallery_dir faces/gallery \
  --top_k 5
```

### 4.4 çŸ¥è¯†è’¸é¦

```bash
python tools/train_rec_kd.py \
  --yolo_images dataset/train \
  --yolo_labels dataset/train \
  --data_yaml dataset/data.yaml \
  --save_dir output/kd \
  --epochs 50 \
  --teacher_torchvision \
  --w_kd_embed 1.0 \
  --w_triplet 1.0 \
  --device cuda
```

---

## 5. Backboneæ¨¡å‹åº“

### 5.1 CNNç³»åˆ—ï¼ˆ60+ä¸ªï¼‰

#### ResNetå®¶æ—
```python
# ResNet
resnet18, resnet34, resnet50, resnet101, resnet152

# ResNeXt
resnext50_32x4d, resnext101_32x8d

# Wide ResNet
wide_resnet50_2, wide_resnet101_2, wide_resnet28_10

# SE-ResNet
se_resnet50, se_resnet101
```

#### è½»é‡çº§æ¨¡å‹
```python
# MobileNet
mobilenet_v2, mobilenet_v3_small, mobilenet_v3_large

# EfficientNet
efficientnet_b0, efficientnet_b1, ..., efficientnet_b7

# GhostNet
ghostnet

# ShuffleNet
shufflenet_v2_x0_5, shufflenet_v2_x1_0
```

#### äººè„¸è¯†åˆ«ä¸“ç”¨
```python
# MobileFaceNet
mobilefacenet

# IR-Net
ir_net_50, ir_net_100, ir_net_152
```

#### é«˜çº§CNN
```python
# DenseNet
densenet121, densenet161, densenet169, densenet201

# DLA (Deep Layer Aggregation)
dla34, dla60

# DPN (Dual Path Networks)
dpn68, dpn92

# Inception
inception_v3

# Xception
xception
```

### 5.2 Transformerç³»åˆ—ï¼ˆ25+ä¸ªï¼‰

```python
# Vision Transformer
vit_tiny_patch16_224, vit_small_patch16_224, vit_base_patch16_224

# Swin Transformer
swin_tiny_patch4_window7_224, swin_small_patch4_window7_224

# DeiT
deit_tiny_patch16_224, deit_small_patch16_224

# ConvNeXt
convnext_tiny, convnext_small, convnext_base

# MobileViT
mobilevit_s, mobilevit_xs

# CSWin Transformer
cswin_tiny, cswin_small

# LeViT
levit_128, levit_256

# PVT-V2
pvt_v2_b0, pvt_v2_b1
```

### 5.3 ä½¿ç”¨ç¤ºä¾‹

```python
from visionhub.ptcls.arch.backbone import build_backbone

# æ„å»ºæ¨¡å‹
model = build_backbone('resnet50', num_classes=1000, pretrained=True)

# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹
from visionhub.ptcls.arch.backbone import list_backbones
all_models = list_backbones()
print(f"Total {len(all_models)} models available")

# æŒ‰ç±»åˆ«ç­›é€‰
cnn_models = [m for m in all_models if 'resnet' in m or 'efficientnet' in m]
transformer_models = [m for m in all_models if 'vit' in m or 'swin' in m]
```

---

## 6. Losså‡½æ•°åº“

### 6.1 åº¦é‡å­¦ä¹ Lossï¼ˆ28ä¸ªï¼‰

#### åŸºç¡€åº¦é‡Loss
```python
from visionhub.ptcls.loss.metric import (
    ArcFaceLoss,      # ArcFace
    CosFaceLoss,      # CosFace  
    SphereFaceLoss,   # SphereFace
    TripletLoss,      # Triplet Loss
    CenterLoss,       # Center Loss
    ContrastiveLoss,  # Contrastive Loss
)

# ä½¿ç”¨ç¤ºä¾‹
criterion = ArcFaceLoss(
    in_features=512,
    num_classes=1000,
    s=64.0,
    m=0.5
)
loss = criterion(embeddings, labels)
```

#### é«˜çº§åº¦é‡Loss
```python
from visionhub.ptcls.loss.metric import (
    CircleLoss,       # Circle Loss
    SupConLoss,       # Supervised Contrastive
    ProxyNCALoss,     # Proxy-NCA
    LiftedStructure,  # Lifted Structure
)

# é«˜çº§åº¦é‡Lossï¼ˆæ–°å¢ï¼‰
from visionhub.ptcls.loss.metric.advanced_metric_loss import (
    MSMLoss,          # Multi-Similarity Mining
    XBMLoss,          # Cross-Batch Memory
    SoftTripleLoss,   # Soft Triple
    AngularLoss,      # Angular Loss
)
```

### 6.2 è’¸é¦Lossï¼ˆ16ä¸ªï¼‰

```python
from visionhub.ptcls.loss.distillation import (
    KLDivLoss,        # KLæ•£åº¦
    DKDLoss,          # Decoupled KD
    RKDLoss,          # Relational KD
)

# é«˜çº§è’¸é¦Lossï¼ˆæ–°å¢ï¼‰
from visionhub.ptcls.loss.distillation.advanced_distill_loss import (
    AFDLoss,          # Attention Feature Distillation
    ReviewKDLoss,     # Review KD
    CRDLoss,          # Contrastive Representation Distillation
)

# ä½¿ç”¨ç¤ºä¾‹
criterion = AFDLoss(attention_type='spatial')
loss = criterion(student_features, teacher_features)
```

### 6.3 åˆ†ç±»Lossï¼ˆ4ä¸ªï¼‰

```python
from visionhub.ptcls.loss import (
    FocalLoss,        # Focal Loss
    LabelSmoothingCrossEntropy,  # Label Smoothing
    AsymmetricLoss,   # Asymmetric Loss (å¤šæ ‡ç­¾)
)
```

### 6.4 Lossç»„åˆä½¿ç”¨

```python
# å¤šLossç»„åˆ
total_loss = (
    1.0 * triplet_loss(embeddings, labels) +
    0.5 * circle_loss(embeddings, labels) +
    1.0 * arcface_loss(embeddings, labels)
)
```

---

## 7. æ•°æ®å¢å¼º

### 7.1 åŸºç¡€å¢å¼º

```python
from torchvision import transforms

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomGrayscale(p=0.1),
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
```

### 7.2 é«˜çº§å¢å¼º

```python
from visionhub.ptcls.data.augmentation.advanced_augment import (
    AutoAugment,
    RandAugment,
    GridMask,
    RandomErasing
)

# AutoAugment
transform = transforms.Compose([
    transforms.Resize(256),
    AutoAugment(policy='imagenet'),
    transforms.ToTensor()
])

# RandAugment
transform = transforms.Compose([
    transforms.Resize(256),
    RandAugment(n=2, m=10),
    transforms.ToTensor()
])
```

### 7.3 æ··åˆå¢å¼º

```python
from visionhub.ptcls.data.augmentation.advanced_augment import (
    mixup_data_enhanced,
    cutmix_data_enhanced,
    fmix
)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for images, labels in dataloader:
    # Mixup
    images, y_a, y_b, lam = mixup_data_enhanced(images, labels, alpha=1.0)
    outputs = model(images)
    loss = lam * criterion(outputs, y_a) + (1-lam) * criterion(outputs, y_b)
    
    # CutMix
    images, y_a, y_b, lam = cutmix_data_enhanced(images, labels, alpha=1.0)
    
    # FMix
    images, y_a, y_b, lam = fmix(images, labels, alpha=1.0, shape=(224, 224))
```

---

## 8. è®­ç»ƒæŒ‡å—

### 8.1 æ ‡å‡†åˆ†ç±»è®­ç»ƒ

**å®Œæ•´è®­ç»ƒè„šæœ¬**:
```bash
python tools/train_classification.py \
  --data_root ./data/imagenet \
  --model resnet50 \
  --num_classes 1000 \
  --epochs 100 \
  --batch_size 128 \
  --lr 0.1 \
  --scheduler cosine \
  --mixup 0.2 \
  --cutmix 0.2 \
  --label_smoothing 0.1 \
  --weight_decay 1e-4 \
  --device cuda \
  --amp \
  --save_dir ./output/resnet50
```

**è®­ç»ƒç›‘æ§**:
```python
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f output/resnet50/train.log

# TensorBoardå¯è§†åŒ–
tensorboard --logdir output/resnet50/tensorboard
```

### 8.2 æ£€ç´¢æ¨¡å‹è®­ç»ƒ

**å®Œæ•´è®­ç»ƒæµç¨‹**:
```bash
# Step 1: è®­ç»ƒæ£€ç´¢æ¨¡å‹
python tools/train_rec_kd.py \
  --yolo_images dataset/train \
  --yolo_labels dataset/train \
  --data_yaml dataset/data.yaml \
  --save_dir output/retrieval \
  --epochs 50 \
  --batch_size 32 \
  --lr 0.001 \
  --use_pk --P 8 --K 4 \
  --w_triplet 1.0 \
  --w_circle 0.2 \
  --teacher_torchvision \
  --device cuda \
  --amp

# Step 2: æ„å»ºæ£€ç´¢åº“
python tools/build_gallery.py \
  -c configs/shitu/rec_faiss_demo.yaml

# Step 3: è¯„ä¼°
python tools/eval_retrieval.py \
  -c configs/shitu/rec_faiss_demo.yaml \
  --gallery_images dataset/val \
  --gallery_labels dataset/val \
  --query_images dataset/test \
  --query_labels dataset/test \
  --data_yaml dataset/data.yaml
```

### 8.3 äººè„¸è¯†åˆ«è®­ç»ƒ

```bash
python tools/train_face_recognition.py \
  --train_root faces/train \
  --val_pairs faces/val/pairs.txt \
  --model ir_net_50 \
  --loss arcface \
  --s 64.0 \
  --m 0.5 \
  --epochs 100 \
  --batch_size 128 \
  --lr 0.1 \
  --scheduler step \
  --weight_decay 5e-4 \
  --save_dir output/face
```

---

## 9. YOLOé›†æˆ

### 9.1 YOLO + åˆ†ç±»

```python
from visionhub.ptcls.tools.yolo_det_classification import YOLODetectionClassification

# åˆ›å»ºç³»ç»Ÿ
system = YOLODetectionClassification(
    yolo_model_path='yolov8n.pt',
    cls_model_name='resnet50',
    cls_checkpoint='classifier.pth',
    num_classes=100,
    class_names=['cat', 'dog', ...]
)

# å•å¼ å›¾ç‰‡
results = system.detect_and_classify('image.jpg', save_result=True)

# æ‰¹é‡å¤„ç†
results = system.batch_predict('images/', save_dir='results/')
```

### 9.2 YOLO + æ£€ç´¢

**å®Œæ•´æµç¨‹**:
```bash
# 1. ç”¨YOLOæ•°æ®è®­ç»ƒæ£€ç´¢æ¨¡å‹
python run_pipeline.py all \
  -c configs/shitu/rec_faiss_demo.yaml \
  --data_yaml dataset/data.yaml \
  --yolo_train_images dataset/train \
  --yolo_train_labels dataset/train \
  --eval_gallery_images dataset/val \
  --eval_gallery_labels dataset/val \
  --eval_query_images dataset/test \
  --eval_query_labels dataset/test \
  --save_dir output/yolo_retrieval \
  --epochs 50 \
  --device cuda

# 2. ä½¿ç”¨æ£€ç´¢ç³»ç»Ÿ
python tools/predict_system.py \
  -c configs/shitu/rec_faiss_demo.yaml \
  --infer_img demo.jpg \
  --save_path result.jpg
```

### 9.3 æ•°æ®æ ¼å¼ï¼ˆYOLOï¼‰

**YOLOæ£€æµ‹æ•°æ®æ ¼å¼**:
```
dataset/
  train/
    img1.jpg
    img1.txt  # class_id x_center y_center width height (å½’ä¸€åŒ–)
    img2.jpg
    img2.txt
  val/
  test/
  data.yaml  # names: [class1, class2, ...]
```

**data.yamlç¤ºä¾‹**:
```yaml
train: dataset/train
val: dataset/val
test: dataset/test

nc: 80  # ç±»åˆ«æ•°
names: ['person', 'bicycle', 'car', ...]
```

---

## 10. éƒ¨ç½²æŒ‡å—

### 10.1 æ¨¡å‹å¯¼å‡º

#### ONNXå¯¼å‡º
```bash
python tools/export_model.py \
  --model resnet50 \
  --checkpoint output/best.pth \
  --num_classes 1000 \
  --format onnx \
  --simplify \
  --save_dir deploy/models
```

#### TensorRTå¯¼å‡º
```bash
# Step 1: å¯¼å‡ºONNX
python tools/export_model.py \
  --model resnet50 \
  --checkpoint output/best.pth \
  --format onnx \
  --save_dir deploy/models

# Step 2: æ„å»ºTensorRTå¼•æ“
python deploy/tensorrt_predictor.py \
  --mode build \
  --onnx deploy/models/best.onnx \
  --engine deploy/models/best.engine \
  --fp16
```

### 10.2 æ¨ç†

#### ONNXæ¨ç†
```bash
python deploy/onnx_predictor.py \
  --model deploy/models/best.onnx \
  --image test.jpg \
  --device cuda \
  --benchmark
```

#### TensorRTæ¨ç†
```bash
python deploy/tensorrt_predictor.py \
  --mode predict \
  --engine deploy/models/best.engine \
  --image test.jpg
```

### 10.3 æ¨¡å‹é‡åŒ–

```bash
python tools/quantize_model.py \
  --model resnet50 \
  --checkpoint output/best.pth \
  --method static \
  --calib_data calibration_images/ \
  --calib_images 100 \
  --output deploy/quantized/best_int8.pth
```

### 10.4 HTTPæœåŠ¡

```bash
python deploy/http_server.py \
  --model resnet50 \
  --checkpoint output/best.pth \
  --num_classes 1000 \
  --class_names classes.txt \
  --host 0.0.0.0 \
  --port 8080
```

**APIè°ƒç”¨**:
```python
import requests

# åˆ†ç±»
files = {'image': open('test.jpg', 'rb')}
response = requests.post('http://localhost:8080/classify', files=files)
print(response.json())
```

### 10.5 æ‰¹é‡æ¨ç†

```bash
python deploy/batch_inference.py \
  --task classify \
  --model resnet50 \
  --checkpoint output/best.pth \
  --image_dir test_images/ \
  --batch_size 32 \
  --output results.json
```

---

## 11. APIå‚è€ƒ

### 11.1 æ ¸å¿ƒAPI

```python
# æ„å»ºæ¨¡å‹
from visionhub.ptcls.arch.backbone import build_backbone
model = build_backbone(name='resnet50', num_classes=1000, pretrained=True)

# æ„å»ºLoss
from visionhub.ptcls.loss.metric import ArcFaceLoss
criterion = ArcFaceLoss(in_features=512, num_classes=1000)

# æ•°æ®åŠ è½½
from visionhub.ptcls.data.datasets import ImageFolderDataset
dataset = ImageFolderDataset(root='data/train', transform=transform)

# è¯„ä¼°æŒ‡æ ‡
from visionhub.ptcls.metric import accuracy, recall_at_k
acc = accuracy(outputs, labels)
recall = recall_at_k(similarity_matrix, labels, k=5)
```

### 11.2 é…ç½®æ–‡ä»¶

**YAMLé…ç½®ç¤ºä¾‹**:
```yaml
# configs/custom_config.yaml
Global:
  device: cuda
  epochs: 100
  batch_size: 128

Arch:
  name: resnet50
  pretrained: True
  num_classes: 1000

Loss:
  Train:
    - CELoss:
        weight: 1.0
    - TripletLoss:
        weight: 0.5
        margin: 0.3

Optimizer:
  name: SGD
  lr: 0.1
  momentum: 0.9
  weight_decay: 0.0001

DataLoader:
  Train:
    dataset:
      name: ImageFolder
      root: ./data/train
    batch_size: 128
    num_workers: 4
```

---

## 12. å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©åˆé€‚çš„Backboneï¼Ÿ

**æ¨èé€‰æ‹©**:
- **é«˜ç²¾åº¦**ï¼šResNet50/101, EfficientNet-B4/B5, Swin-Transformer
- **é€Ÿåº¦ä¼˜å…ˆ**ï¼šMobileNetV3, EfficientNet-B0, ResNet18
- **äººè„¸è¯†åˆ«**ï¼šIR-Net-50/100, MobileFaceNet
- **æ£€ç´¢ä»»åŠ¡**ï¼šResNet50 + Embedding Head

### Q2: YOLOæ•°æ®èƒ½ç›´æ¥ç”¨äºè®­ç»ƒå—ï¼Ÿ

**å¯ä»¥ï¼** visionhubæ”¯æŒç›´æ¥ä½¿ç”¨YOLOæ£€æµ‹æ•°æ®ï¼š
```bash
python tools/train_classification.py \
  --yolo_images dataset/train \
  --yolo_labels dataset/train \
  --data_yaml dataset/data.yaml \
  --model resnet50 \
  --epochs 100
```

ç³»ç»Ÿä¼šè‡ªåŠ¨cropæ£€æµ‹æ¡†ä½œä¸ºåˆ†ç±»æ ·æœ¬ã€‚

### Q3: è®­ç»ƒæ•ˆæœä¸å¥½æ€ä¹ˆåŠï¼Ÿ

**ä¼˜åŒ–å»ºè®®**:
1. å¢åŠ æ•°æ®å¢å¼ºï¼š`--mixup 0.2 --cutmix 0.2`
2. ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼š`--pretrained`
3. è°ƒæ•´å­¦ä¹ ç‡ï¼š`--lr 0.01 --scheduler cosine`
4. ä½¿ç”¨æ›´å¼ºLossï¼šArcFace, Circle Loss
5. å¢åŠ è®­ç»ƒè½®æ•°ï¼š`--epochs 200`

### Q4: å¦‚ä½•åŠ é€Ÿæ¨ç†ï¼Ÿ

**åŠ é€Ÿæ–¹æ¡ˆ**:
1. ä½¿ç”¨TensorRTï¼š`5-10xåŠ é€Ÿ`
2. æ¨¡å‹é‡åŒ–ï¼š`INT8é‡åŒ–ï¼Œ4xå‹ç¼©`
3. ä½¿ç”¨è½»é‡æ¨¡å‹ï¼šMobileNet, EfficientNet-B0
4. æ‰¹é‡æ¨ç†ï¼šå¢å¤§batch_size

### Q5: æ”¯æŒå¤šGPUè®­ç»ƒå—ï¼Ÿ

**æ”¯æŒï¼**
```bash
# ä½¿ç”¨DataParallel
python -m torch.distributed.launch --nproc_per_node=4 \
  tools/train_classification.py \
  --data_root data/ \
  --model resnet50 \
  --distributed
```

---

## ğŸ“ è·å–å¸®åŠ©

- **æ–‡æ¡£**: [https://visionhub.readthedocs.io](docs/)
- **Issue**: [GitHub Issues](https://github.com/cs405/visionhub/issues)
- **ç¤ºä¾‹**: [examples/](examples/)

---

**âœ… æ–‡æ¡£å®Œæˆï¼visionhubå·²å‡†å¤‡å°±ç»ªï¼** ğŸ‰

